{
	"name": "Lab 07 - Part 2 - Spark Delta Lake",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPool01",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2
		},
		"metadata": {
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 60
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Delta Lake features"
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Use `spark.read.csv()` to load the data from the source public blob storage account and display its schema and shape."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"import numpy as np\r\n",
					"import pandas as pd\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"manualSchema = StructType([\r\n",
					"  StructField(\"CustomerId\", StringType(), True),\r\n",
					"  StructField(\"ProductId\", StringType(), True),\r\n",
					"  StructField(\"Rating\", LongType(), True),\r\n",
					"  StructField(\"Cost\", FloatType(), True),\r\n",
					"  StructField(\"Size\", FloatType(), True),\r\n",
					"  StructField(\"Price\", FloatType(), True),\r\n",
					"  StructField(\"PrimaryBrandId\", LongType(), True),\r\n",
					"  StructField(\"GenderId\", LongType(), True),\r\n",
					"  StructField(\"MaritalStatus\", LongType(), True),\r\n",
					"  StructField(\"LowerIncomeBound\", FloatType(), True),\r\n",
					"  StructField(\"UpperIncomeBound\", FloatType(), True)\r\n",
					"])\r\n",
					"\r\n",
					"url = \"wasbs://files@synapsemlpublic.blob.core.windows.net/PersonalizedData.csv\"\r\n",
					"raw_data = spark.read.csv(url, header=True, schema=manualSchema)\r\n",
					"print(\"Schema: \")\r\n",
					"raw_data.printSchema()\r\n",
					"\r\n",
					"df = raw_data.toPandas()\r\n",
					"print(\"Shape: \", df.shape)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Save the customer rating dataframe as a Delta Lake table."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"delta_table_path = 'abfss://delta@asadatalake641162.dfs.core.windows.net/customer-rating'\r\n",
					"\r\n",
					"raw_data.write.format('delta').save(delta_table_path)\r\n",
					"\r\n",
					"mssparkutils.fs.ls(delta_table_path)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore the layout of files and inspect the Delta log file."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"delta_log_path = mssparkutils.fs.ls(f'{delta_table_path}/_delta_log')[0].path\r\n",
					"print(delta_log_path)\r\n",
					"mssparkutils.fs.head(delta_log_path)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Load the Delta lake table into a Spark dataframe."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"data = spark.read.format('delta').load(delta_table_path)\r\n",
					"data.show()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Use the dedicated `DeltaTable` class to manage the Delta Lake table. Explore the Delta Lake table history."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Get all versions\r\n",
					"delta_table = DeltaTable.forPath(spark, delta_table_path)\r\n",
					"display(delta_table.history())"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Perform an update on the Delta Lake table using a SQL-style condition."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Declare the predicate by using a SQL-formatted string.\r\n",
					"delta_table.update(\r\n",
					"  condition = \"Price < 1500\",\r\n",
					"  set = { \"Price\": \"Price * 1.05\" }\r\n",
					")\r\n",
					""
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Check again the history of the Delta Lake table and notice the new entry corresponding to the update that has just been performed."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(delta_table.history())"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"It's possible to query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(delta_table_path))"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(delta_table_path))\r\n",
					""
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Create the metadata to expose the Delta Lake table in the default Spark database."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"CREATE TABLE CustomerRating USING DELTA LOCATION '{0}'\".format(delta_table_path))"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"List all tables that exist in the default Spark database."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"SHOW TABLES\").show()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore the properties of the `CustomerRating` Spark database table."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"DESCRIBE EXTENDED customerrating\").show(truncate=False)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"To query the delta table from the serverless SQL pool, navigate to the `Develop` hub in Synapse Studio and create a new SQL script. Make sure `Built-in` is selected for the `Connect to` option and `default` is selected for the `Use database` option.\r\n",
					"\r\n",
					"Enter the query below and make sure you replace `<your_data_lake_account_name>` with the name of the Data Lake account with the one from your lab environment.\r\n",
					"\r\n",
					"\r\n",
					"```sql\r\n",
					"SELECT TOP 10 *\r\n",
					"FROM OPENROWSET(\r\n",
					"    BULK 'abfss://delta@<your_data_lake_account_name>.dfs.core.windows.net/customer-rating/',\r\n",
					"    FORMAT = 'delta') as rows\r\n",
					")\r\n",
					"```\r\n",
					"\r\n",
					"![Query Delta Lake with serverless SQL pool](https://solliancepublicdata.blob.core.windows.net/synapse-l400/notebook-images/query-delta-table.png)"
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This concludes the Delta Lake section of this notebook.\r\n",
					"\r\n",
					"To learn more about Delta Lake support in Syanspe Spark, take a look at the [Work with Delta Lake](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-delta-lake-overview?pivots=programming-language-python) section in the Azure Synapse Analytics documentation."
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Mount storage account containers\r\n",
					"\r\n",
					"The `mssparkutils` utility can be used to mount storage account containers. In the example below, you will use an already created linked service to manage the authentication with the storage account."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"mssparkutils.fs.mount( \r\n",
					"    \"abfss://delta@asadatalake641162.dfs.core.windows.net\", \r\n",
					"    \"/test\", \r\n",
					"    {\"linkedService\":\"asadatalake641162\"} \r\n",
					") "
				],
				"attachments": null,
				"execution_count": 60
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore the content of the mounted volume using the local path. Note the `synfs:/{jobId}` prefix used by `mssparkutils` for the local path."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"jobId = mssparkutils.env.getJobId() \r\n",
					"\r\n",
					"log_files = mssparkutils.fs.ls(f'synfs:/{jobId}/test/customer-rating/_delta_log')\r\n",
					"log_files"
				],
				"attachments": null,
				"execution_count": 61
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"When using regular PySpark classes, the syntax of the prefix is slightly different - `/synfs/{jobId}`. Use this prefix to load and display the first 500 characters from the first Delta Lake log file."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"with open(f'/synfs/{jobId}/test/customer-rating/_delta_log/{log_files[0].name}', 'r') as f:\r\n",
					"    f.read()[:500]"
				],
				"attachments": null,
				"execution_count": 62
			}
		]
	}
}